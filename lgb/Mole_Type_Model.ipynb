{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import scipy.signal as sg\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook focuses on merging all good datasets and modeling. The datasets ending with fc_sd_pso_dso_SCC_1 only contain molecular level features without type specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole = pd.read_csv('E:/kaggle/Molecular_properties/Data_use/train_20190820_fc_sd_pso_dso_SCC_1.csv')\n",
    "test_mole = pd.read_csv('E:/kaggle/Molecular_properties/Data_use/test_20190820_fc_sd_pso_dso_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "test_mole.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022400000000005</td>\n",
       "      <td>0.254579</td>\n",
       "      <td>1.25862</td>\n",
       "      <td>0.272010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.034700000000001</td>\n",
       "      <td>0.352978</td>\n",
       "      <td>2.85839</td>\n",
       "      <td>-3.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.032500000000001</td>\n",
       "      <td>0.352944</td>\n",
       "      <td>2.85852</td>\n",
       "      <td>-3.433870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.031900000000000</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>2.85855</td>\n",
       "      <td>-3.433930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dsgdb9nsd_000001</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>83.022199999999998</td>\n",
       "      <td>0.254585</td>\n",
       "      <td>1.25861</td>\n",
       "      <td>0.272013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      molecule_name  atom_index_0  atom_index_1  type                  fc  \\\n",
       "0  dsgdb9nsd_000001             1             0  1JHC  83.022400000000005   \n",
       "1  dsgdb9nsd_000001             1             2  2JHH -11.034700000000001   \n",
       "2  dsgdb9nsd_000001             1             3  2JHH -11.032500000000001   \n",
       "3  dsgdb9nsd_000001             1             4  2JHH -11.031900000000000   \n",
       "4  dsgdb9nsd_000001             2             0  1JHC  83.022199999999998   \n",
       "\n",
       "         sd      pso       dso  \n",
       "0  0.254579  1.25862  0.272010  \n",
       "1  0.352978  2.85839 -3.433600  \n",
       "2  0.352944  2.85852 -3.433870  \n",
       "3  0.352934  2.85855 -3.433930  \n",
       "4  0.254585  1.25861  0.272013  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar_coupling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_coupling = pd.read_csv('E:/kaggle/Molecular_properties/champs-scalar-coupling/scalar_coupling_contributions.csv')\n",
    "#scalar_coupling.head()\n",
    "y_fc = scalar_coupling['fc']\n",
    "y_sd = scalar_coupling['sd']\n",
    "y_pso = scalar_coupling['pso']\n",
    "y_dso = scalar_coupling['dso']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load df_molecules for later group k fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_molecules = pd.read_csv('E:/kaggle/Molecular_properties/Data_use/molecules.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to implement a way to calculate group mae\n",
    "def train_model_regression(X, X_test, y, params, folds, molecules, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, groups = molecules)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "gkf = GroupKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_merge(df, df_merge, both_on):\n",
    "    df = pd.merge(df, df_merge, how = 'left',\n",
    "                 left_on = both_on,\n",
    "                 right_on = both_on)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuplicateColumns(df):\n",
    "    duplicateColumnNames = set()\n",
    "    goodCol = set()\n",
    "    \n",
    "    for x in range(df.shape[1]):\n",
    "        col = df.iloc[:, x]\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            otherCol = df.iloc[:, y]\n",
    "            if col.equals(otherCol):\n",
    "                found = True\n",
    "                goodCol.add(df.columns.values[x])\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "            \n",
    "                \n",
    "    return list(duplicateColumnNames), list(goodCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking features by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short = pd.DataFrame({'ind': list(train_mole.index), 'type': train_mole['type'].values, \n",
    "                        'oof_fc': [0] * len(train_mole), 'oof_sd': [0] * len(train_mole), 'oof_pso': [0] * len(train_mole), 'oof_dso': [0] * len(train_mole),\n",
    "                        'target_fc': y_fc.values, 'target_sd': y_sd.values, 'target_pso': y_pso.values, 'target_dso': y_dso.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(test_mole.index), 'type': test_mole['type'].values, \n",
    "                             'prediction_fc': [0] * len(test_mole), 'prediction_sd': [0] * len(test_mole), 'prediction_pso': [0] * len(test_mole), 'prediction_dso': [0] * len(test_mole) })\n",
    "types_iterate = train_mole['type'].unique()\n",
    "len(types_iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_secondary(X_short_secondary, X_short_test_secondary, params, features_toget, types_iterate_index, model_type = 'lgb'):\n",
    "    \n",
    "    # Good molecular level features\n",
    "    df_features = pd.read_csv(f'E:/kaggle/Molecular_properties/good_features/good_ft_type{types_iterate_index}_bonds_QM9_inver_neighbor_huber.csv')\n",
    "    good_features = list(df_features['feature'].values) + ['molecule_name', 'atom_index_0', 'atom_index_1']\n",
    "    type_index = types_iterate[types_iterate_index]\n",
    "    \n",
    "    temp = train_mole[['type', 'type_string']].loc[train_mole['type'] == type_index].head()\n",
    "    Type = temp['type_string'].unique()[0]\n",
    "    X_t = train_mole[good_features].loc[train_mole['type'] == type_index]\n",
    "    X_test_t = test_mole[good_features].loc[test_mole['type'] == type_index]\n",
    "    old_index = X_t.index\n",
    "    old_index_test = X_test_t.index\n",
    "    \n",
    "    # Dataset with base neighbor features for a type\n",
    "    X_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/train_more_neighbor_20190805/train_{Type}.csv')\n",
    "    X_test_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/test_more_neighbor_20190809/test_{Type}.csv')\n",
    "        \n",
    "    X_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    X_test_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    \n",
    "    X_t = Data_merge(X_t, X_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    X_test_t = Data_merge(X_test_t, X_test_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    \n",
    "    # Dataset with inverse distance related neighbor features for a type\n",
    "    X_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/train_20190822/train_{Type}.csv')\n",
    "    X_test_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/test_20190822/test_{Type}.csv')\n",
    "\n",
    "    X_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    X_test_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "    X_t = Data_merge(X_t, X_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    X_test_t = Data_merge(X_test_t, X_test_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "\n",
    "    X_t.index = old_index\n",
    "    X_test_t.index = old_index_test\n",
    "    \n",
    "    bad_features, _ = getDuplicateColumns(X_t[:40000])\n",
    "    bad_features = bad_features + ['molecule_name', 'atom_index_0', 'atom_index_1', 'cosinus', 'dihedral',\n",
    "                                   'type_string', 'scalar_coupling_constant']\n",
    "    good_features = [col for col in X_t.columns.values if col not in bad_features]\n",
    "    \n",
    "    print(f'Training of type {Type}')\n",
    "    \n",
    "    for feature in features_toget:\n",
    "        \n",
    "        print('Training of feature ' + feature)\n",
    "        label = 'target_' + feature\n",
    "        train_secondary_feature = 'oof_' + feature\n",
    "        test_secondary_feature = 'prediction_' + feature\n",
    "        \n",
    "        y_t = X_short_secondary.loc[X_short['type'] == type_index, label]\n",
    "        molecules_t = df_molecules.loc[df_molecules['type'] == type_index, 'molecule_name']\n",
    "        \n",
    "        result_dict_lgb_oof = train_model_regression(X=X_t[good_features], X_test=X_test_t[good_features], y=y_t, params=params, molecules = molecules_t,\n",
    "                                                                  folds=gkf, model_type=model_type, eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                                  verbose=1000, early_stopping_rounds=500, n_estimators=20000)\n",
    "\n",
    "        X_short_secondary.loc[X_short_secondary['type'] == type_index, train_secondary_feature] = result_dict_lgb_oof['oof']\n",
    "        X_short_test_secondary.loc[X_short_test_secondary['type'] == type_index, test_secondary_feature] = result_dict_lgb_oof['prediction']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof: -0.5362\n",
    "params = {'num_leaves': 300,                           #initial 200       best 300 with max_depth 25                                   \n",
    "          'min_child_samples': 60,                      #initial 79       best 60         \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 25,                            # initial none    \n",
    "          'colsample_bytree': 0.5,                    # initial 0.9      best 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # initial 0.25    best0.08        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 2,                            #initial 0.1         best 2         \n",
    "          'reg_lambda': 0.3                          #initial 0.3                            \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc', 'pso', 'dso'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof: -2.1969\n",
    "params = {'num_leaves': 300,                           #initial 200         best 300                           \n",
    "          'min_child_samples': 60,                      #initial 79         best 60 \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 30,                              #initial none       best 30      \n",
    "          'colsample_bytree': 0.5,                    # 0.9                 best 0.3\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25              best 0.05   \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1         best 0.3       \n",
    "          'reg_lambda': 0.3                       #initial 0.3                                \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc', 'sd', 'pso'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof: -1.2694\n",
    "params = {'num_leaves': 300,                           #initial 200    best: 300                                 \n",
    "          'min_child_samples': 60,                      #initial 79    best: 60        \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 60,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9           best  0.18\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.1,                        # 0.25        best: 0.04\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.5,                            #initial 0.1    best: 0.5   \n",
    "          'reg_lambda': 0.5                           #initial 0.3     best: 0.5                            \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc', 'pso', 'dso'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof: -1.9993\n",
    "params = {'num_leaves': 300,                           #initial 200       best: 300                              \n",
    "          'min_child_samples': 79,                      #initial 79          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # initial 0.9       best: 0.5\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25            best: 0.05        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1       \n",
    "          'reg_lambda': 0.3                           #initial 0.3                               \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc', 'sd', 'pso'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1.5059\n",
    "params = {'num_leaves': 300,                           #initial 200      best: 300                               \n",
    "          'min_child_samples': 50,                      #initial 79      best: 50    \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9              best: 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25           best: 0.05        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.75,                            #initial 0.1        best: 0.75     \n",
    "          'reg_lambda': 0.2                           #initial 0.3       best: 0.2                              \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 300,                           #initial 200            best: 300                         \n",
    "          'min_child_samples': 60,                      #initial 79            best: 60          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9                    best: 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25                 best: 0.05   \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.3,                            #initial 0.1            best: 0.3\n",
    "          'reg_lambda': 1                           #initial 0.3               best: 1                                 \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1.5768\n",
    "params = {'num_leaves': 300,                           #initial 200        best: 300                             \n",
    "          'min_child_samples': 60,                      #initial 79        best: 60          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9                best: 0.5\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25             best: 0.05\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1        best: 0.05   \n",
    "          'reg_lambda': 0.3                          #initial 0.3                                     \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -2.4587\n",
    "params = {'num_leaves': 300,                           #initial 200       best: 300                              \n",
    "          'min_child_samples': 79,                      #initial 79       best: 30   \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.6,                    # 0.9               best: 0.6\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25            best: 0.03\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1       best: 0.1   \n",
    "          'reg_lambda': 0.3                           #initial 0.3                                 \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_secondary(X_short, X_short_test, params, ['fc', 'sd', 'pso'], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole['oof_fc'] = X_short['oof_fc']\n",
    "test_mole['oof_fc'] = X_short_test['prediction_fc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole['oof_sd'] = X_short['oof_sd']\n",
    "test_mole['oof_sd'] = X_short_test['prediction_sd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole['oof_pso'] = X_short['oof_pso']\n",
    "test_mole['oof_pso'] = X_short_test['prediction_pso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole['oof_dso'] = X_short['oof_dso']\n",
    "test_mole['oof_dso'] = X_short_test['prediction_dso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mole.to_csv('E:/kaggle/Molecular_properties/Data_use/train_20190826_fc_sd_pso_dso_SCC_1.csv')\n",
    "test_mole.to_csv('E:/kaggle/Molecular_properties/Data_use/test_20190826_fc_sd_pso_dso_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = train_mole['scalar_coupling_constant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "gkf = GroupKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short = pd.DataFrame({'ind': list(train_mole.index), 'type': train_mole['type'].values, \n",
    "                        'oof_lgb': [0] * len(train_mole), 'oof_xgb': [0] * len(train_mole), 'oof_ridge': [0] * len(train_mole),\n",
    "                        'target': y_tr.values\n",
    "                       })\n",
    "X_short_test = pd.DataFrame({'ind': list(test_mole.index), 'type': test_mole['type'].values, \n",
    "                             'prediction_lgb': [0] * len(test_mole), 'prediction_xgb': [0] * len(test_mole), 'prediction_ridge': [0] * len(test_mole),\n",
    "                            })\n",
    "types_iterate = train_mole['type'].unique()\n",
    "len(types_iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_byType(X_short, X_short_test, params, features_secondary, types_iterate_index, model_type = 'lgb'):\n",
    "    \n",
    "    df_features = pd.read_csv(f'E:/kaggle/Molecular_properties/good_features/good_ft_type{types_iterate_index}_bonds_QM9_inver_neighbor_huber.csv')\n",
    "    good_features = list(df_features['feature'].values) + ['molecule_name', 'atom_index_0', 'atom_index_1'] + features_secondary\n",
    "    type_index = types_iterate[types_iterate_index]\n",
    "    \n",
    "    temp = train_mole[['type', 'type_string']].loc[train_mole['type'] == type_index].head()\n",
    "    Type = temp['type_string'].unique()[0]\n",
    "    X_t = train_mole[good_features].loc[train_mole['type'] == type_index]\n",
    "    X_test_t = test_mole[good_features].loc[test_mole['type'] == type_index]\n",
    "    old_index = X_t.index\n",
    "    old_index_test = X_test_t.index\n",
    "    \n",
    "    # Dataset with base neighbor features for a type\n",
    "    X_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/train_more_neighbor_20190805/train_{Type}.csv')\n",
    "    X_test_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/test_more_neighbor_20190809/test_{Type}.csv')\n",
    "        \n",
    "    X_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    X_test_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    \n",
    "    X_t = Data_merge(X_t, X_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    X_test_t = Data_merge(X_test_t, X_test_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    \n",
    "    # Dataset with inverse distance related neighbor features for a type\n",
    "    X_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/train_20190822/train_{Type}.csv')\n",
    "    X_test_t_merge = pd.read_csv(f'E:/kaggle/Molecular_properties/test_20190822/test_{Type}.csv')\n",
    "\n",
    "    X_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    X_test_t_merge.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "\n",
    "    X_t = Data_merge(X_t, X_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "    X_test_t = Data_merge(X_test_t, X_test_t_merge, ['molecule_name', 'atom_index_0', 'atom_index_1'])\n",
    "\n",
    "    X_t.index = old_index\n",
    "    X_test_t.index = old_index_test\n",
    "    \n",
    "    bad_features, _ = getDuplicateColumns(X_t[:40000])\n",
    "    bad_features = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type_string', 'scalar_coupling_constant']\n",
    "    good_features = [col for col in X_t.columns.values if col not in bad_features]\n",
    "    \n",
    "    print(f'Training of type {Type}')\n",
    "         \n",
    "    y_t = X_short.loc[X_short['type'] == type_index, 'target']\n",
    "    molecules_t = df_molecules.loc[df_molecules['type'] == type_index, 'molecule_name']\n",
    "\n",
    "    result_dict_lgb_oof = train_model_regression(X=X_t[good_features], X_test=X_test_t[good_features], y=y_t, params=params, molecules = molecules_t,\n",
    "                                                              folds=gkf, model_type=model_type, eval_metric='group_mae', plot_feature_importance=False,\n",
    "                                                              verbose=1000, early_stopping_rounds=500, n_estimators=20000)\n",
    "\n",
    "    X_short.loc[X_short['type'] == type_index, 'oof_' + model_type] = result_dict_lgb_oof['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == type_index, 'prediction_' + model_type] = result_dict_lgb_oof['prediction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber' w/o fc: -0.4507  -0.5219  -0.4878  -0.4583  -0.4657  -0.5196\n",
    "# Directly adding up: -0.4923\n",
    "# -0.1191   fc: -0.7834  fc+sd:-0.7934  fc+sd+pso:-0.8027  fc+sd+pso+dso: -0.8020  fc+sd+dso: -0.8016 fc+pso+dso: -0.8049\n",
    "# fc+pso: -0.7920  fc+dso: -0.7907\n",
    "params = {'num_leaves': 300,                           #initial 200       best 300 with max_depth 25                                   \n",
    "          'min_child_samples': 60,                      #initial 79       best 60         \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 25,                            # initial none    \n",
    "          'colsample_bytree': 0.5,                    # initial 0.9      best 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # initial 0.25    best0.08        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 2,                            #initial 0.1         best 2         \n",
    "          'reg_lambda': 0.3                          #initial 0.3                             \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc', 'oof_pso', 'oof_dso'],\n",
    "             0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w 'huber' fc: -2.1603  -2.2533  -2.2741  -2.2167   -2.1698   -2.1626   -2.2412\n",
    "# Directly adding up: -2.1452\n",
    "# -1.7509   fc: -2.1915  fc+sd: -2.1785  fc+pso: -2.1775  fc+dso:-2.1770  fc+sd+pso:-2.2183  fc+sd+dso:-2.2142\n",
    "# fc+pso+dso: -2.2122  fc+sd+pso+dso: -2.1922\n",
    "params = {'num_leaves': 300,                           #initial 200         best 300                           \n",
    "          'min_child_samples': 60,                      #initial 79         best 60 \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 30,                              #initial none       best 30      \n",
    "          'colsample_bytree': 0.5,                    # 0.9                 best 0.3\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25              best 0.05   \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1         best 0.3       \n",
    "          'reg_lambda': 0.3                       #initial 0.3                            \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc', 'oof_sd', 'oof_pso'],\n",
    "             1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w'huber' fc: -1.1385  -1.2825  -1.3014  -1.1448  -1.1761   -1.2116  -1.2695\n",
    "# Directly adding up: -1.205\n",
    "# -1.1427  fc: -1.1232  fc+sd: -1.1252  fc+pso: -1.1319  fc+dso:-1.1390  fc+sd+pso: -1.1284  fc+sd+dso: -1.1416  \n",
    "# fc+pso+dso: -1.1490  fc+sd+pso+dso: -1.1452\n",
    "params = {'num_leaves': 300,                           #initial 200    best: 300                                 \n",
    "          'min_child_samples': 60,                      #initial 79    best: 60        \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': 60,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9           best  0.18\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.1,                        # 0.25        best: 0.04\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.5,                            #initial 0.1    best: 0.5   \n",
    "          'reg_lambda': 0.5                           #initial 0.3     best: 0.5                             \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc', 'oof_pso', 'oof_dso'],\n",
    "             2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w'huber' fc: -1.9772   -2.0570   -2.0614  -2.0227  -1.9779   -1.9795   -2.0444\n",
    "# Directly adding up: -1.9754\n",
    "# -1.7188   fc: -2.0483  fc+sd: -2.0388   fc+pso: -2.0458   fc+dso: -2.0463  fc+sd+pso: -2.0831  fc+sd+sdo: -2.0766\n",
    "# fc+pso+dso: -2.0805    fc+sd+pso+dso: -2.0585\n",
    "params = {'num_leaves': 300,                           #initial 200       best: 300                              \n",
    "          'min_child_samples': 79,                      #initial 79          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # initial 0.9       best: 0.5\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25            best: 0.05        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1       \n",
    "          'reg_lambda': 0.3                           #initial 0.3                             \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc', 'oof_sd', 'oof_pso'], \n",
    "             3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w 'huber' fc: -1.4041  -1.5036   -1.5201  -1.4618   -1.4238  -1.4485  -1.5344\n",
    "# Directly adding up: -1.4583\n",
    "# -0.9357   fc: -1.6998  fc+sd: -1.6735   fc+pso: -1.675  fc+dso: -1.6717   fc+sd+pso: -1.6402  fc+pso+dso: -1.6360\n",
    "# fc+sd+dso:  -1.6448   fc+sd+pso+dso: -1.6644\n",
    "params = {'num_leaves': 300,                           #initial 200      best: 300                               \n",
    "          'min_child_samples': 50,                      #initial 79      best: 50    \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9              best: 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25           best: 0.05        \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.75,                            #initial 0.1        best: 0.75     \n",
    "          'reg_lambda': 0.2                           #initial 0.3       best: 0.2                       \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc'],\n",
    "             4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w/o fc: -2.0471  -2.1918  -2.1648  -2.0844   -2.1487   -2.1846\n",
    "# Directly adding up: -2.0848\n",
    "# -1.6215   fc: -2.2635  fc+sd: -2.2167  fc+pso: -2.2094   fc+dso: -2.2155  fc+sd+pso: -2.1836  fc+sd+dso: -2.1854\n",
    "# fc+pso+dso: -2.1777   fc+sd+pso+dso: -2.2149\n",
    "params = {'num_leaves': 300,                           #initial 200            best: 300                         \n",
    "          'min_child_samples': 60,                      #initial 79            best: 60          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9                    best: 0.45\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25                 best: 0.05   \n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.3,                            #initial 0.1            best: 0.3\n",
    "          'reg_lambda': 1                           #initial 0.3               best: 1                       \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc'],\n",
    "             5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w 'huber' fc: -1.4907   -1.5805  -1.5873  -1.5469  -1.5208  -1.5772   -1.6105\n",
    "# Directly adding up: -1.5225\n",
    "# -0.9887   fc: -1.8958   fc+sd: -1.8101    fc+pso: -1.8033   fc+dso: -1.8063   fc+sd+pso: -1.7794  fc+sd+dso: -1.7717\n",
    "# fc+pso+dso: -1.7790   fc+sd+pso+dso: -1.8105 \n",
    "params = {'num_leaves': 300,                           #initial 200        best: 300                             \n",
    "          'min_child_samples': 60,                      #initial 79        best: 60          \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.5,                    # 0.9                best: 0.5\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25             best: 0.05\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1        best: 0.05   \n",
    "          'reg_lambda': 0.3                          #initial 0.3                               \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc'],\n",
    "             6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'huber': w 'huber' fc: -2.3953    -2.5190  -2.5509  -2.4635   -2.4176   -2.4609    -2.4917\n",
    "# Directly adding up: -2.4219\n",
    "#-2.1084  fc: -2.5922  fc+sd: -2.5682   fc+pso: -2.5739  fc+dso: -2.5705  fc+sd+pso: -2.5953   fc+sd+dso: -2.5870 \n",
    "# fc+pso+dso: -2.5931  fc+sd+pso+dso: -2.5855\n",
    "params = {'num_leaves': 300,                           #initial 200       best: 300                              \n",
    "          'min_child_samples': 79,                      #initial 79       best: 30   \n",
    "          'objective': 'huber',                                        \n",
    "          #'max_depth': -1,                                    \n",
    "          'colsample_bytree': 0.6,                    # 0.9               best: 0.6\n",
    "          'subsample': 0.8,                            #initial 0.8\n",
    "          'learning_rate': 0.15,                        # 0.25            best: 0.03\n",
    "          \"metric\": 'mae',                                       \n",
    "          'reg_alpha': 0.1,                            #initial 0.1       best: 0.1   \n",
    "          'reg_lambda': 0.3                           #initial 0.3                                \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_byType(X_short, X_short_test, params, \n",
    "             ['oof_fc', 'oof_sd', 'oof_pso'],\n",
    "             7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short_test.to_csv('E:/kaggle/Molecular_properties/X_short_test_20190815.csv')\n",
    "X_short.to_csv('E:/kaggle/Molecular_properties/X_short_20190815.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('E:/kaggle/Molecular_properties/champs-scalar-coupling/sample_submission.csv', index_col='id')\n",
    "benchmark = sample_submission.copy()\n",
    "benchmark.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>18.660577573957198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>196.566977602471354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>9.952218486467974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>196.565609161362687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>18.508949659251101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147        18.660577573957198\n",
       "1  4658148       196.566977602471354\n",
       "2  4658149         9.952218486467974\n",
       "3  4658150       196.565609161362687\n",
       "4  4658151        18.508949659251101"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark['scalar_coupling_constant'] = X_short_test['prediction_lgb']\n",
    "benchmark.to_csv('train_type_feature_20190828.csv', index=False)\n",
    "benchmark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2505537</th>\n",
       "      <td>7163684</td>\n",
       "      <td>2.649353092376293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505538</th>\n",
       "      <td>7163685</td>\n",
       "      <td>4.151215380486076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505539</th>\n",
       "      <td>7163686</td>\n",
       "      <td>2.356377834792806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505540</th>\n",
       "      <td>7163687</td>\n",
       "      <td>2.608921942259303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2505541</th>\n",
       "      <td>7163688</td>\n",
       "      <td>122.051770281175990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  scalar_coupling_constant\n",
       "2505537  7163684         2.649353092376293\n",
       "2505538  7163685         4.151215380486076\n",
       "2505539  7163686         2.356377834792806\n",
       "2505540  7163687         2.608921942259303\n",
       "2505541  7163688       122.051770281175990"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blend results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_1 = pd.read_csv('E:/kaggle/Molecular_properties/results/train_type_feature_20190828.csv', index_col='id')\n",
    "submission_2 = pd.read_csv('E:/kaggle/Molecular_properties/results/train_type_feature_20190825_2.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_3 = submission_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.77\n",
    "submission_3['scalar_coupling_constant'] = (submission_1['scalar_coupling_constant'] * ratio + submission_2['scalar_coupling_constant'] * (1 - ratio))\n",
    "submission_3.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>18.621610632002060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>196.612846917043868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>9.831053684298507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>196.586967229371766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>18.402404777816415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147        18.621610632002060\n",
       "1  4658148       196.612846917043868\n",
       "2  4658149         9.831053684298507\n",
       "3  4658150       196.586967229371766\n",
       "4  4658151        18.402404777816415"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_3.to_csv('train_type_feature_20190828_blending_3.csv', index=False)\n",
    "submission_3.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
